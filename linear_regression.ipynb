{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b62b16",
   "metadata": {},
   "source": [
    "#### 지도학습\n",
    "- training data (input : 정답 Set)을 통해 학습 => 미지의 데이터에 대한 **미래 값 예측**\n",
    "- 회귀(regression): 연속적 값 예측\n",
    "- 분류(classification): 이산적 분류값 예측\n",
    "\n",
    "#### 비지도학습\n",
    "- training data ( only input )을 통해 학습 => 입력 데이터의 **패턴, 특성**을 발견"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf59e9c",
   "metadata": {},
   "source": [
    "#### Linear regression\n",
    "- y = wx + b (w: weight, b: bias)를 찾는 과정 => 오차 (t-y)를 최소화\n",
    "- 손실함수: sigma (t-y)^2 / n => **경사하강법**: 손실함수의 최소값을 갖는 w, b를 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ee8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multi_var_numerical_derivative import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c84c96",
   "metadata": {},
   "source": [
    "#### Single variable regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bdd3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "x_data = np.arange(1, 6).reshape(5,1)\n",
    "t_data = np.arange(2, 7).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab63674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear function y = Wx + b\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a56fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50195239]]\n",
      "[0.24758474]\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dceb46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    return np.sum( (y - t)**2 ) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ba1c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value =  5.543126074303357\n",
      "initial W =  [[0.50195239]]  b =  [0.24758474]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2 # if diverge, change rate to 1e-3, 1e-4, ... \n",
    "\n",
    "print(\"initial error value = \", loss_func(x_data, t_data))\n",
    "print(\"initial W = \", W, \" b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ecad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : loss_func(x_data, t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d6758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: W =  [[0.65666778]]  b =  [0.28323298]  error = 3.286937\n",
      "STEP 200: W =  [[1.08442003]]  b =  [0.69529151]  error = 0.016900\n",
      "STEP 400: W =  [[1.0424288]]  b =  [0.84685607]  error = 0.004269\n",
      "STEP 600: W =  [[1.02132436]]  b =  [0.92303115]  error = 0.001078\n",
      "STEP 800: W =  [[1.01071744]]  b =  [0.9613161]  error = 0.000272\n",
      "STEP 1000: W =  [[1.0053865]]  b =  [0.9805578]  error = 0.000069\n",
      "STEP 1200: W =  [[1.00270721]]  b =  [0.99022851]  error = 0.000017\n",
      "STEP 1400: W =  [[1.00136062]]  b =  [0.99508893]  error = 0.000004\n",
      "STEP 1600: W =  [[1.00068384]]  b =  [0.99753174]  error = 0.000001\n",
      "STEP 1800: W =  [[1.00034369]]  b =  [0.99875947]  error = 0.000000\n",
      "STEP 2000: W =  [[1.00017274]]  b =  [0.99937652]  error = 0.000000\n",
      "STEP 2200: W =  [[1.00008682]]  b =  [0.99968664]  error = 0.000000\n",
      "STEP 2400: W =  [[1.00004363]]  b =  [0.99984251]  error = 0.000000\n",
      "STEP 2600: W =  [[1.00002193]]  b =  [0.99992085]  error = 0.000000\n",
      "STEP 2800: W =  [[1.00001102]]  b =  [0.99996022]  error = 0.000000\n",
      "STEP 3000: W =  [[1.00000554]]  b =  [0.99998001]  error = 0.000000\n",
      "STEP 3200: W =  [[1.00000278]]  b =  [0.99998995]  error = 0.000000\n",
      "STEP 3400: W =  [[1.0000014]]  b =  [0.99999495]  error = 0.000000\n",
      "STEP 3600: W =  [[1.0000007]]  b =  [0.99999746]  error = 0.000000\n",
      "STEP 3800: W =  [[1.00000035]]  b =  [0.99999872]  error = 0.000000\n",
      "STEP 4000: W =  [[1.00000018]]  b =  [0.99999936]  error = 0.000000\n",
      "STEP 4200: W =  [[1.00000009]]  b =  [0.99999968]  error = 0.000000\n",
      "STEP 4400: W =  [[1.00000004]]  b =  [0.99999984]  error = 0.000000\n",
      "STEP 4600: W =  [[1.00000002]]  b =  [0.99999992]  error = 0.000000\n",
      "STEP 4800: W =  [[1.00000001]]  b =  [0.99999996]  error = 0.000000\n",
      "STEP 5000: W =  [[1.00000001]]  b =  [0.99999998]  error = 0.000000\n",
      "STEP 5200: W =  [[1.]]  b =  [0.99999999]  error = 0.000000\n",
      "STEP 5400: W =  [[1.]]  b =  [0.99999999]  error = 0.000000\n",
      "STEP 5600: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 5800: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 6000: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 6200: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 6400: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 6600: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 6800: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 7000: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 7200: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 7400: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 7600: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 7800: W =  [[1.]]  b =  [1.]  error = 0.000000\n",
      "STEP 8000: W =  [[1.]]  b =  [1.]  error = 0.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(8001):\n",
    "    # 경사 하강법\n",
    "    W -= learning_rate * multi_var_numerical_derivative(f, W) \n",
    "    b -= learning_rate * multi_var_numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 200 == 0):\n",
    "        print(\"STEP %d:\"  % (step), \"W = \", W, \" b = \", b, \" error = %f\" % loss_func(x_data, t_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036e3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lambda x : np.dot(W, x) + b # predict function after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11d14d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.]]\n"
     ]
    }
   ],
   "source": [
    "print(predict(43))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40564825",
   "metadata": {},
   "source": [
    "#### Multi variable regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fd75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.  80.  75. 152.]\n",
      " [ 93.  88.  93. 185.]\n",
      " [ 89.  91.  90. 180.]\n",
      " [ 96.  98. 100. 196.]\n",
      " [ 73.  66.  70. 142.]\n",
      " [ 53.  46.  55. 101.]\n",
      " [ 69.  74.  77. 149.]\n",
      " [ 47.  56.  60. 115.]\n",
      " [ 87.  79.  90. 175.]\n",
      " [ 79.  70.  88. 164.]\n",
      " [ 69.  70.  73. 141.]\n",
      " [ 70.  65.  74. 141.]\n",
      " [ 93.  95.  91. 184.]\n",
      " [ 79.  80.  73. 152.]\n",
      " [ 70.  73.  78. 148.]\n",
      " [ 93.  89.  96. 192.]\n",
      " [ 78.  75.  68. 147.]\n",
      " [ 81.  90.  93. 183.]\n",
      " [ 88.  92.  86. 177.]\n",
      " [ 78.  83.  77. 159.]\n",
      " [ 82.  86.  90. 177.]\n",
      " [ 86.  82.  89. 175.]\n",
      " [ 78.  83.  85. 175.]\n",
      " [ 76.  83.  71. 149.]\n",
      " [ 96.  93.  95. 192.]]\n",
      "x_data.ndim =  2 , x_data.shape =  (25, 3)\n",
      "t_data.ndim =  2 , t_data.shape =  (25, 1)\n"
     ]
    }
   ],
   "source": [
    "training_data = np.loadtxt('./multi_var_regression_trainig_data_1.csv', dtype = np.float32, delimiter = \",\")\n",
    "x_data = training_data[:, 0:-1]\n",
    "t_data = training_data[:, [-1]] # t_data = ~[:, -1]로 했더니 vector가 만들어져 loss_func에서 오류 발생. t_data도 matrix로 생성해야.\n",
    "\n",
    "# 데이터 차원 및 shape 확인\n",
    "print(training_data)\n",
    "print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape)\n",
    "print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f90e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear function f = W1x1 + W2x2 + W3x3 + b\n",
    "W = np.random.rand(3,1)\n",
    "b = np.random.rand(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b394a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value =  2267.356017874453\n",
      "initial W =  [[0.2130198 ]\n",
      " [0.9281377 ]\n",
      " [0.29859428]]  b =  [0.20076654]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5 # if diverge, change rate \n",
    "\n",
    "print(\"initial error value = \", loss_func(x_data, t_data))\n",
    "print(\"initial W = \", W, \" b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66024df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : loss_func(x_data, t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b75447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: W =  [[0.28870685]\n",
      " [1.00399606]\n",
      " [0.37677082]]  b =  [0.201337]  error = 852.651962\n",
      "STEP 400: W =  [[0.39894321]\n",
      " [1.02508068]\n",
      " [0.59943938]]  b =  [0.20219645]  error = 17.770230\n",
      "STEP 800: W =  [[0.39281194]\n",
      " [0.94392199]\n",
      " [0.68470867]]  b =  [0.20206164]  error = 14.288056\n",
      "STEP 1200: W =  [[0.38759711]\n",
      " [0.87603054]\n",
      " [0.75612354]]  b =  [0.20183524]  error = 11.848042\n",
      "STEP 1600: W =  [[0.38315725]\n",
      " [0.81924054]\n",
      " [0.81593775]]  b =  [0.20153214]  error = 10.138267\n",
      "STEP 2000: W =  [[0.3793732 ]\n",
      " [0.77173948]\n",
      " [0.86603817]]  b =  [0.20116482]  error = 8.940172\n",
      "STEP 2400: W =  [[0.37614454]\n",
      " [0.73201048]\n",
      " [0.90800453]]  b =  [0.20074372]  error = 8.100610\n",
      "STEP 2800: W =  [[0.37338659]\n",
      " [0.69878418]\n",
      " [0.94315949]]  b =  [0.20027759]  error = 7.512275\n",
      "STEP 3200: W =  [[0.37102792]\n",
      " [0.6709983 ]\n",
      " [0.97261044]]  b =  [0.19977375]  error = 7.099976\n",
      "STEP 3600: W =  [[0.36900825]\n",
      " [0.6477639 ]\n",
      " [0.99728457]]  b =  [0.19923833]  error = 6.811027\n",
      "STEP 4000: W =  [[0.36727664]\n",
      " [0.62833713]\n",
      " [1.0179582 ]]  b =  [0.19867648]  error = 6.608512\n",
      "STEP 4400: W =  [[0.36579006]\n",
      " [0.61209555]\n",
      " [1.03528138]]  b =  [0.19809249]  error = 6.466563\n",
      "STEP 4800: W =  [[0.36451213]\n",
      " [0.59851832]\n",
      " [1.04979839]]  b =  [0.19748998]  error = 6.367054\n",
      "STEP 5200: W =  [[0.36341203]\n",
      " [0.58716964]\n",
      " [1.06196499]]  b =  [0.19687195]  error = 6.297284\n",
      "STEP 5600: W =  [[0.36246368]\n",
      " [0.5776849 ]\n",
      " [1.07216282]]  b =  [0.19624094]  error = 6.248353\n",
      "STEP 6000: W =  [[0.36164498]\n",
      " [0.569759  ]\n",
      " [1.08071146]]  b =  [0.19559906]  error = 6.214026\n",
      "STEP 6400: W =  [[0.36093716]\n",
      " [0.56313672]\n",
      " [1.08787855]]  b =  [0.19494809]  error = 6.189931\n",
      "STEP 6800: W =  [[0.36032429]\n",
      " [0.55760452]\n",
      " [1.09388821]]  b =  [0.19428952]  error = 6.173008\n",
      "STEP 7200: W =  [[0.35979286]\n",
      " [0.55298377]\n",
      " [1.09892813]]  b =  [0.19362458]  error = 6.161111\n",
      "STEP 7600: W =  [[0.35933133]\n",
      " [0.54912504]\n",
      " [1.10315553]]  b =  [0.19295434]  error = 6.152735\n",
      "STEP 8000: W =  [[0.35892991]\n",
      " [0.54590332]\n",
      " [1.10670206]]  b =  [0.19227965]  error = 6.146828\n",
      "STEP 8400: W =  [[0.35858023]\n",
      " [0.54321408]\n",
      " [1.10967801]]  b =  [0.19160125]  error = 6.142651\n",
      "STEP 8800: W =  [[0.35827518]\n",
      " [0.54096986]\n",
      " [1.11217576]]  b =  [0.19091976]  error = 6.139686\n",
      "STEP 9200: W =  [[0.35800864]\n",
      " [0.53909754]\n",
      " [1.11427268]]  b =  [0.19023569]  error = 6.137570\n",
      "STEP 9600: W =  [[0.35777542]\n",
      " [0.53753596]\n",
      " [1.1160336 ]]  b =  [0.18954947]  error = 6.136051\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    # 경사 하강법\n",
    "    W -= learning_rate * multi_var_numerical_derivative(f, W) \n",
    "    b -= learning_rate * multi_var_numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"STEP %d:\"  % (step), \"W = \", W, \" b = \", b, \" error = %f\" % loss_func(x_data, t_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3baba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lambda x : np.dot(x, W) + b # predict function after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b44ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179.01549884]]\n"
     ]
    }
   ],
   "source": [
    "print(predict(np.array([100,98,81]).reshape(1,3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
